{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a00ab4",
   "metadata": {},
   "source": [
    "# dBB type detection with downsized DNA barcode libraries, to estimate the required sequencing depth (relevant to Figure S7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a80aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.covariance\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ad\n",
    "import statistics\n",
    "from scipy.stats import gmean\n",
    "from scipy.spatial import distance\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3da021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBBSeqLabeler:\n",
    "    def __init__(self, counts, min_sig_count, alpha=1e-3):  \n",
    "        self.mapped= counts.iloc[:-1]\n",
    "        psuedo_count = 1\n",
    "        self.min_log_std = 1.\n",
    "        self.log_mapped = np.log(self.mapped + psuedo_count)\n",
    "        self.alpha = alpha\n",
    "        self.min_sig_count = min_sig_count\n",
    "\n",
    "    def _fit(self, log_mapped):\n",
    "        cov = sklearn.covariance.EllipticEnvelope(\n",
    "                contamination=0.1,\n",
    "                random_state=0)\n",
    "        cov.fit(log_mapped.values[:, None])\n",
    "        return cov\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self._cov = self._fit(self.log_mapped)\n",
    "            self.bg_loc = np.ravel(self._cov.location_)\n",
    "            self.bg_prec = np.ravel(self._cov.precision_)\n",
    "        except ValueError as e:\n",
    "            self.bg_loc = self.log_mapped.mean()\n",
    "            self.bg_prec = 1./ min(self.log_mapped.std(ddof=1), self.min_log_std)\n",
    "        self.bg_z_values = (self.log_mapped - self.bg_loc) * self.bg_prec\n",
    "        self.bg_probs = np.where(self.mapped >= self.min_sig_count, 1 - scipy.stats.norm.cdf(self.bg_z_values), 1)\n",
    "        self.signals = self.bg_probs <= self.alpha\n",
    "\n",
    "    def get_signals(self):\n",
    "        return self.signals\n",
    "\n",
    "    def get_bg_probs(self):\n",
    "        return self.bg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3beba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBBSeqTableLabeler:\n",
    "    def __init__(self, table, min_sig_count, min_total_count):\n",
    "        \n",
    "        if min_total_count:\n",
    "            table = table.loc[:, table[:-1].sum(axis=0) >= min_total_count]\n",
    "        self.table = table  \n",
    "        self.labelers = [IBBSeqLabeler(self.table[cell], min_sig_count=min_sig_count) for cell in self.table.columns]\n",
    "   \n",
    "    def run(self):\n",
    "\n",
    "        for labeler in tqdm.tqdm(self.labelers):\n",
    "            labeler.run()\n",
    "        self.signals = np.asarray([l.get_signals() for l in self.labelers], dtype=int).T  \n",
    "        self.bg_probs = np.asarray([l.get_bg_probs() for l in self.labelers]).T  \n",
    "\n",
    "    def save(self, out_prefix):\n",
    "        output = out_prefix + '.signals.txt.gz'\n",
    "        tab = pd.DataFrame(self.signals, index=self.table[:-1].index, columns=self.table.columns)\n",
    "        tab.to_csv(output, sep='\\t', compression='gzip')\n",
    "\n",
    "        pos_sel = self.signals.sum(axis=0) > 0\n",
    "        pos_columns = self.table.columns.values[pos_sel]\n",
    "        pos_signals = self.signals[:, pos_sel]\n",
    "        output = out_prefix + '.pos_signals.txt.gz'\n",
    "        tab = pd.DataFrame(pos_signals, index=self.table[:-1].index, columns=pos_columns)\n",
    "        tab.to_csv(output, sep='\\t', compression='gzip')\n",
    "\n",
    "        output = out_prefix + '.bg_probs.txt.gz'\n",
    "        tab = pd.DataFrame(self.bg_probs, index=self.table[:-1].index, columns=self.table[:-1].columns)\n",
    "        tab.to_csv(output, float_format='%.6g', sep='\\t', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a91f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir = 'PATH_TO_FOLDER'    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba8a79",
   "metadata": {},
   "source": [
    "# Reading 4 downsized samples + the original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec2cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = ['Rep2_dBB_0p2M', 'Rep2_dBB_1M', 'Rep2_dBB_5M', 'Rep2_dBB_10M', 'Rep2_dBB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc2b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted by library sizes\n",
    "Min_total_count = [1 ,2 ,10 ,20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a73d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted by library sizes\n",
    "Min_sig_count =  [1 ,2 ,10 ,20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cb0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = \"1e-3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6900df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 7370)\n",
      "(59, 7370)\n",
      "(59, 7367)\n",
      "(59, 7364)\n",
      "(59, 7358)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Data)):\n",
    "    dense_umis = Dir + Data[i] + \"/out/dense_umis.tsv\"\n",
    "    tab = pd.read_csv(dense_umis, sep='\\t', index_col=0)\n",
    "    print(tab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c5200",
   "metadata": {},
   "source": [
    "# Positive signal detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbc3265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 6592/6592 [00:02<00:00, 2562.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_0p2M: The number of data points with at least one dBB type,  6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7256/7256 [00:02<00:00, 2488.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_1M: The number of data points with at least one dBB type,  4592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7286/7286 [00:06<00:00, 1080.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_5M: The number of data points with at least one dBB type,  3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7284/7284 [00:08<00:00, 907.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_10M: The number of data points with at least one dBB type,  3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 7304/7304 [00:08<00:00, 869.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB: The number of data points with at least one dBB type,  3452\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Data)):\n",
    "    dense_umis = Dir + Data[i] + \"/out/dense_umis.tsv\"\n",
    "    tab = pd.read_csv(dense_umis, sep='\\t', index_col=0)\n",
    "    out_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    labeler = IBBSeqTableLabeler(tab, min_total_count=Min_total_count[i], min_sig_count=Min_sig_count[i])\n",
    "    labeler.run()\n",
    "    labeler.save(out_prefix)\n",
    "    test = pd.read_csv(out_prefix + '.pos_signals.txt.gz', sep='\\t', index_col=0) \n",
    "    print(Data[i] + \": The number of data points with at least one dBB type, \" , str(len(test.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b036de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_0p2M: Mean_num_indexes_per_cell, 2.012333965844402\n",
      "Rep2_dBB_1M: Mean_num_indexes_per_cell, 1.8656358885017421\n",
      "Rep2_dBB_5M: Mean_num_indexes_per_cell, 2.144906323185012\n",
      "Rep2_dBB_10M: Mean_num_indexes_per_cell, 2.1224242424242425\n",
      "Rep2_dBB: Mean_num_indexes_per_cell, 2.1891657010428736\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Data)):\n",
    "    dense_umis = Dir + Data[i] + \"/out/dense_umis.tsv\"\n",
    "    tab = pd.read_csv(dense_umis, sep='\\t', index_col=0)\n",
    "    out_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    \n",
    "    test = pd.read_csv(out_prefix + '.pos_signals.txt.gz', sep='\\t', index_col=0)\n",
    "    print(Data[i] + ': Mean_num_indexes_per_cell, ' + str(np.mean(test.apply(lambda x: x[0:58].sum()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13b12f",
   "metadata": {},
   "source": [
    "# Generating combinatorial codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efeae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beads = 3 # minumum positive iBB types per droplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac09a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_0p2M: The number of data points after filtering, 1719\n",
      "Rep2_dBB_1M: The number of data points after filtering, 542\n",
      "Rep2_dBB_5M: The number of data points after filtering, 888\n",
      "Rep2_dBB_10M: The number of data points after filtering, 891\n",
      "Rep2_dBB: The number of data points after filtering, 987\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Data)):\n",
    "    \n",
    "    cond = '3-9_'\n",
    "    dense_umis = Dir + Data[i] + \"/out/dense_umis.tsv\"\n",
    "    tab = pd.read_csv(dense_umis, sep='\\t', index_col=0)\n",
    "    out_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    \n",
    "    test = pd.read_csv(out_prefix + '.pos_signals.txt.gz', sep='\\t', index_col=0)\n",
    "    test = test.loc[:, test.apply(lambda x: x[0:58].sum()) > num_beads-1]\n",
    "    test = test.loc[:, test.apply(lambda x: x[0:58].sum()) < 10] \n",
    "    pos_counts = test.apply(np.sum, axis=0)\n",
    "    unique_ibb_nunits = pos_counts.value_counts().sort_index()\n",
    "    \n",
    "    temp = pd.DataFrame(test.T)\n",
    "    temp['idx'] = 0\n",
    "\n",
    "    print(Data[i] + \": The number of data points after filtering, \" + str(len(temp)))\n",
    "    \n",
    "    for k in range(0, len(temp)):\n",
    "        temp.iloc[k,58] = ''.join(str(s) for s in temp.iloc[k,0:58].tolist())\n",
    "    temp=temp.sort_values(by=['idx'])\n",
    "    temp.iloc[:,58].to_csv(out_prefix  + '_' + cond + 'pattern.txt', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ce18a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ec2ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_12(tab, i1, i2):  # TODO also use color distance?\n",
    "    hamdist = len(list(tab.loc[i1, 'idx'])) * distance.hamming(list(tab.loc[i1, 'idx']), list(tab.loc[i2, 'idx'])) # hamming dist\n",
    "    return (hamdist < dist) \n",
    "\n",
    "def create_adj_matrix(tab):\n",
    "    return np.asarray([[dist_12(tab, i1, i2) for i1 in tab.index] for i2 in tab.index])\n",
    "\n",
    "def group_idx(tab):\n",
    "    mat = create_adj_matrix(tab)\n",
    "    n, components = sparse.csgraph.connected_components(mat, directed=True)\n",
    "    return n, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0afcfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(adata, cond, dist):\n",
    "\n",
    "    idx = pd.read_csv(input_prefix + '_' + cond + 'pattern.txt', sep='\\t')\n",
    "    idx.columns = [\"sc_id\",\"idx\"]\n",
    "\n",
    "    n, clusters = group_idx(idx)\n",
    "    print(\"Num_dropletIDs, \" + str(n))\n",
    "    idx = idx.assign(cluster=clusters)\n",
    "    idx = idx.sort_values('cluster')\n",
    "    idx = idx.loc[:,[\"sc_id\",\"cluster\"]]\n",
    "    \n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(adata.obs.index)):\n",
    "        if idx['sc_id'].isin([adata.obs.index[i]]).any():\n",
    "            label = idx[idx['sc_id']==adata.obs.index[i]]['cluster']\n",
    "            labels.append(label.values[0])\n",
    "        else:\n",
    "            labels.append('no_idx')\n",
    "\n",
    "    print(\"Num_cells_with_dropletIDs:\" + str(len(labels) - labels.count('no_idx')))\n",
    "\n",
    "    adata.obs['idx'] = labels\n",
    "    new = pd.concat([adata.obs['leiden'], adata.obs['K562_score'], adata.obs['THP1_score'], adata.obs['idx']], axis=1)\n",
    "    new = new[new.duplicated(subset='idx', keep = False)]\n",
    "    new = new[new.idx != 'no_idx']\n",
    "    new = new.sort_values('idx')\n",
    "    new.leiden = new.leiden.astype(int)\n",
    "    grouped = new.groupby('idx')\n",
    "    stat = pd.concat([grouped.size(), grouped.mean()], axis=1)\n",
    "    stat.columns = (['num_cells', 'cell_types', 'K562_score ', 'THP1_score '])\n",
    "    stat.to_csv(output_prefix + cond + '_dist' + str(dist-1) + '.txt', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e977c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(data, cond, dist):\n",
    "    \n",
    "    colnames = ['cond','dist','num_units','max_num_cells_per_unit','min_num_cells_per_unit','error_rate']\n",
    "    stats = pd.DataFrame(columns = colnames)  \n",
    "    \n",
    "    data = data\n",
    "    input_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    \n",
    "    stat = pd.read_csv(input_prefix + cond + '_dist' + str(dist-1) + '.txt', sep='\\t')\n",
    "    \n",
    "    identity = []\n",
    "    \n",
    "    for l in range(len(stat.cell_types)):\n",
    "            if ((stat.cell_types.iloc[l] == 0.0)|(stat.cell_types.iloc[l] == 1.0)):\n",
    "                identity.append(1)\n",
    "            else:\n",
    "                identity.append(0)\n",
    "    \n",
    "    stats = pd.concat([stats, pd.DataFrame([[cond, \\\n",
    "                                            dist-1, \\\n",
    "                                            len(stat.cell_types), \\\n",
    "                                            max(stat.num_cells.tolist()), \\\n",
    "                                            min(stat.num_cells.tolist()), \\\n",
    "                                            1-statistics.mean(identity)]],\\\n",
    "                                           columns = colnames)], axis=0)\n",
    "    \n",
    "    stats.to_csv(input_prefix + '_stats.txt', sep='\\t', header=True)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccd6f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_proc = ad.read_h5ad('PATH_TO_h5ad') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4318e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = '3-9_'\n",
    "dist = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bea711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep2_dBB_0p2M\n",
      "Num_dropletIDs, 1689\n",
      "Num_cells_with_dropletIDs:1230\n",
      "Rep2_dBB_1M\n",
      "Num_dropletIDs, 510\n",
      "Num_cells_with_dropletIDs:345\n",
      "Rep2_dBB_5M\n",
      "Num_dropletIDs, 814\n",
      "Num_cells_with_dropletIDs:551\n",
      "Rep2_dBB_10M\n",
      "Num_dropletIDs, 800\n",
      "Num_cells_with_dropletIDs:563\n",
      "Rep2_dBB\n",
      "Num_dropletIDs, 891\n",
      "Num_cells_with_dropletIDs:612\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Data)):\n",
    "    input_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    output_prefix = Dir + Data[i] + '/' + Data[i]\n",
    "    print(Data[i])\n",
    "    matching(adata=adata_proc, cond=cond, dist=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d2736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Colnames = ['cond','dist','num_units','max_num_cells/unit','min_num_cells/unit','error_rate', 'data']\n",
    "stats_summary = pd.DataFrame(columns = Colnames)\n",
    "\n",
    "for i in range(len(Data)):\n",
    "    \n",
    "    data = Data[i]\n",
    "    summary(data=data, cond=cond, dist=dist)\n",
    "    stats = pd.read_csv(Dir + Data[i] + '/' + Data[i] + '_stats.txt', sep='\\t',index_col=0)\n",
    "    stats['data'] = Data[i]\n",
    "    stats.columns = Colnames\n",
    "    stats_summary = pd.concat([stats_summary, stats], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "892f3749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cond</th>\n",
       "      <th>dist</th>\n",
       "      <th>num_units</th>\n",
       "      <th>max_num_cells/unit</th>\n",
       "      <th>min_num_cells/unit</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-9_</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Rep2_dBB_0p2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-9_</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>Rep2_dBB_1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-9_</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>Rep2_dBB_5M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-9_</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>Rep2_dBB_10M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-9_</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>Rep2_dBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cond dist num_units max_num_cells/unit min_num_cells/unit  error_rate  \\\n",
       "0  3-9_    0        16                  2                  2    0.500000   \n",
       "0  3-9_    0        16                  3                  2    0.062500   \n",
       "0  3-9_    0        36                  3                  2    0.111111   \n",
       "0  3-9_    0        39                  3                  2    0.102564   \n",
       "0  3-9_    0        38                  3                  2    0.078947   \n",
       "\n",
       "            data  \n",
       "0  Rep2_dBB_0p2M  \n",
       "0    Rep2_dBB_1M  \n",
       "0    Rep2_dBB_5M  \n",
       "0   Rep2_dBB_10M  \n",
       "0       Rep2_dBB  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73c9163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary.to_csv(Dir + 'Suppl_Rep2_3-9_dist0_error_rates.txt', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77cbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kappa_1",
   "language": "python",
   "name": "kappa_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
